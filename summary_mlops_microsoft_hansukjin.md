# MLOps 소개 ep.0
- MLOps 분야의 전반적인 소개, 특징에 대한 영상
- 마이크로소프트의 Azure Machine Learning 서비스, 한석진 연구자 발표
   - 깃헙 : https://github.com/devrel-kr/azure-playlists/tree/main/2021H1/%ED%95%9C%EC%84%9D%EC%A7%84-AutoML

## 오프닝

### GPT-3 머신러닝 모델
- 언어모델
- 텍스트 빅데이터를 수집하고 모델에 학습시켜서 여러가지 뛰어난 기능을 수행한다.
   - react 앱을 만들어준다
   - sql 코딩을 해준다.
   - 인터페이스 디자인을 제작해준다.
   - 수식을 만들어 준다. 
   - 리눅스 코딩을 해준다.
   - 엑셀에서 추론을 해준다.
   - 이 머신러닝 모델로 대화를 해보면 사람하고 대화하는 것같은 기능이 뛰어나다.

### 가장 간단한 ML도 데이터에서 출발한다.
- 리니어 레그레션과 같이 데이터가 기초가된다.

### 머신러닝 모델 개발은 다양한 과정이 결합된 것과 같다.
- ``개발 과정``
	- **데이터 가져오기**
	- **데이터 탐색**
	- **데이터 가공**
	- **데이터 검증**
	- **데이터 나누기**
	- **학습방법 결정**
	- **모델 만들기**
	- **모델 검증**
	- **대규모 학습**
	- **대규모 배포 준비**
	- **배포와 운영**
	- **로깅**
	- **모니터링**
	
## MLops가 뭐길래? ep.1

### 생애주기
- 문제정의부터 서비스의 배포와 모니터링까지를 포함한다. 
	- 문제정의 접근방법 검토
	- 데이터가져오기, 데이터탐색, 데이터가공, 데이터검증, 데이터나누기
	- 학습방법 결정, 모델만들기, 모델검증, 대규모학습
	- 대규모 배포준비, 배포와 운영, 로깅, 모니터링
	- 데이터 가져오기로 다시 반복

### MLOps 누가하나? Actor
- ``MLOps 두개의 축``
   - 개발과 서빙의 축
- **데이터 사이언티스트 관점**
   - 빠르게 실험반복
   - 원하는 ML/DL
   - 프레임워크사용
   - 가장 좋은 tool
   - 머리아픈 관리는 최소화
   - 대용량 : 데이터 가공, 모델학습 
- **데이터 엔니지어, 소프트 엔지니어 관점**
   - tool과 플랫폼의 재사용
   - 전사 정책
   - 모니터링, 감시
   - 죽지 않고 살아남기
- 두 그룹다 주로 고려하는 부분들이 다름
   - 서로 이해를 할 수록 성공적인 프로젝트가 된다.

### 일반적인 DevOps와의 차이점
- ML 기반 시스템의 테스트와 모니터링
- DebOps
- **MLOps**
   - 데브옵스와 크게 보면 비슷함
   - research / production : 데브옵스와 가장 큰 차이점
   - 데이터에서 모델을 만드는 과정 + 패징해서 서비스 배포 하는 과정으로 나뉜다.
   - 모델을 만들기 위한 데이터를 공급하는 과정
   - 만든 모델을 운영서비스상으로 배포를 한 후 방금 받은 새로운 데이터가 공급되는 과정
- **MLOps = ML + DEV + OPS**
   - ML 실험 : 비즈니스 이해, 데이터탐색/가공, 초기모델링, 최정화/평가
   - Develop 개발 : 지속적인 통합(continuous integration) - 빌드(모델최적화), 테스트, 지속적인 제공(continous deliver) - 모델등록
   - Operate 운영 : 지속적인 배포(continuous deployment) - 서빙, 데이터 피드백 루프 - data collection, 시스템/모델 모니터링 - data drift 모니터링

### MLOps가 뭐가 좋을까?
- 왜하지? 장점 : **MLOps는 ML계의 Rapid Developments**
   - "안정적이고 빠르게" 개발하는 과정이 프로젝트의 성공을 좌우한다.
- 성공적인 배포서비스를 위해서는 잘 챙겨야 할 것들이 많다. 이것을 효율적으로 관리해주는 도구, 프레임웤이라고 볼 수 있다.
- 비즈니스/현업 관점
   - 변화관리에 장점 : 툴 + 일하는 방식
   - 안하던 것을 하는 것
   - 운영수준이 레벨업된다.
- IT/개발 부서의 관점
   - 기존에 잘 하던 품질관리 수준을 ML에도 적용할 수 있다.
   - ML 프로세스의 수준이 향상된다. : 소스, 통합테스트, 컴파일/배포, 모니터링
- 데이터전문가
   - **내가 만든 모델이 배포과정까지 안정적으로 전달이 되는지, 모니터링을 할 수 있다.**
   - 파이프라인 자동화 : 데이터가공 
   - ML을 production 데이터와 연계
   - 지속적인 모니터링
   - 재학습 / 재배포 자동화

### MLOps 어디까지 가능할까?
- MLOps Framework : 슬라이드 참조

## ML생애주기 데이터 준비 ep.2

### 문제/데이터 정의, 가설 수립
- ``문제정의/가설수립 단계``
   - 풀고 싶은 문제를 정의한다. : 제대로 된 질문하기
   - 질문에 대한 답을 찾기 위해 가설을 여러개 수립한다.
   - 가설을 검증하기 위한 데이터 셋들이 필요. : 데이터 확보
   - 비즈니스 임팩트 고려할 사항

### 데이터 이동과 연계 : 데이터셋 공유 및 재사용
- 데이터 확보, 준비 단계 : 데이터가져오기, 데이터탐색, 데이터가공, 데이터검증
   - **데이터 확보** : 데이터셋 확보방안, 파일서버, 클라우드 스토리지 등에 저장되어 있다. 
   - **데이터 수집** : 실시간 또는 배치형태로 가져올 수 있다.
   - **데이터 연계** : 데이터 유형에 따라서 다른 방식으로 연계한다. 쿼리, 엑셀 등의 방법으로 데이터를 하나로 만드는 것.
      - 이미지를 종류별로 나눠주는 작업도 데이터 연계를 준비하는 과정이라고 볼 수 있다.
      - 텍스트 모델이라면 그 모델이 필요한 특정 영역의 데이터를 준비하는 과정
- **데이터셋 공유 및 재사용**
   - **버전관리** : 데이터를 어디서 가져왔고 이것을 어떤 버전으로 관리할 것인지 중요, 어제버전, 오늘버전, 데이터 추가된 버전 등
   - 누적되는 데이터 중 특정 범위를 논리적으로 관리
   - 어떤 실험에서 사용됐는지 연결 관리
   - 마이크로소프트 azure 편하게 데이터를 들여다 볼 수 있는 기능이 있다.

### 데이터 탐색과 가공 
- ``데이터 탐색 : 데이터를 들여다 보고 빠진값, 클래스간 균형 등의 사항들도 데이터에 대한 탐색과정이라고 볼 수 있다.``
   - 오픈소스에 데이터 탐색이 가능한 툴이 제공된다. : 쥬피터 노트북, R 등.. 
   - 데이터 탐색은 다른 어떤 단계보다 선행한다.
- **데이터 레이블링**
   - 지도학습의 경우 : supervised learning
   - 레이블 : 목표값이 주어져야 한다.
   - 돌고래, 문어, 상어가 있는 사진에 대한 분류작업 : 레이블을 달아주는 작업자체가 학습데이터를 준비하는 과정
- azure 에서 데이터 레이블링 기능 있음 : 이미지를 종류별로 구분해주는 기능이 있음
   - 이미지 segmentation : 이미지에서 특정 부분을 따로 떼어내 레이블링 해줌
   - 블라우스 인지 아닌지 이미지 분류가능
   - 텍스트 레이블링 : 네이버 감성분석 데이터 비슷한 기능, 긍정이냐 부정이냐 등의 레이블링을 쉽게 할 수 있다.
- **Feature importance 탐색**
   - feature : 컬럼
   - 어떤 컬럼이 더 중요하고, 덜 중요하느냐?
   - 데이터의 패턴, 분포를 보고 어떤 컬럼들이 같이 움직이는지 탐색하는 과정
   - azure 에 피쳐 탐색 기능 : 어떤 피쳐가 중요하고, 나중에 학습데이터로 사용할 수 있게 분류 기능 등

## ML 생애주기 실험/학습 ep.3

### 실험, 모델 학습 / 최적화 / 비교평가
- ``실험/학습 과정 : 데이터 가공, 데이터 검증, 데이터 나누기, 학습방법 결정, 모델 만들기, 모델 검증, 대규모 학습``
   - 데이터를 어떻게 가공하느냐에 따라서 모델의 성능이 달라지므로 포함됨.
- 모델 개발에 있어서 입력 요소 : 데이터셋, 트레이닝 알고리즘(데이터안에서 패턴을 찾아냄), 하이퍼파라미터 조정
   - 이러한 입력 요소를 가지고 모델을 구축한다. 
   - 우리가 원하는 모델이 바로 만들어지지 않는다. 
   - 입력 조건을 바꿔가면서 최적의 머신러닝 모델을 찾는 과정이다. : 그러므로 실험과 같다.
   - 알고리즘을 바꾸고, 하이퍼파라미터를 바꾸면 또 다른 모델이 나온다. 데이터를 바꾸면 이에 맞는 또 다른 모델이 나온다.
- ``이러한 과정을 어떻게 최적화 할 것인가?``
- 비교평가
   - 여러 모델을 비교
   - 최근 3개 모델은 어떤 알고리즘/파라미터로 학습했더라? : 파라미터 기록 관리
   - feature 변경은 어떤 영향이 있었지? 
   - 지난번에 썼던 스크립트는 어떤 버전이었고 그때 썼던 상세 로그는 어떤거였지?
- **실험, 학습, 평가의 단계에서 여러가지 데이터가 쌓이고 이것을 잘 관리해야 좋은 성능의 모델을 만들 수 있다.**

### 실험 추적관리
- ``실험의 조건들 : code 버젼, dataset 버전, 개발환경(python 패키지 버전들)``
- model의 버전
- 등록된 모델이 배포 됨 : 배포 된 후 예측 결과에 대한 데이터가 쌓인다.
- 실행 이력의 관리 : 출력, 메트릭, 스냅샷 등의 데이터가 쌓인다.
- 이러한 버전, 데이터 들을 추적 관리 해주어야 한다. 
- azure 의 실험 추적관리 기능
   - 당뇨병 데이터로 여러가지 모델을 만들었고, 이 모델들의 성능을 저장할 수 있다.
   - 어떤 데이터 셋을 사용했는지 링크 정보 등
   - 모델이 어떤 파이프라인으로 어떤 머신러닝 모델로 만들어졌는지 정보를 등록할 수 있다.
   - 서비스로 배포 했는지도 등록할 수 있다. 

### 자동화된 ML Automated ML
- 입력데이터 - 알고리즘과 파라미터 지정 - 모델 개발
- 이 과정을 자동화한 것 : 어떻게 효율적으로 자동화 할 것인가
   - 입력데이터와 파이프라인, 파라미터 값에 따라 모델의 성능이 어떨것이라는 예측도 가능하다. 
   - 최적의 모델을 만드는 과정에서 반복적으로 수행되는 과정을 미리 예측하고 최적화할 수 있다.
- 어떤 모델이 성능이 좋은지, 성능에 대한 지표, metric 을 통해서 파악 가능
- **스냅샷** : 어떤 로직으로 실험이 됐는지 쉽게 파악할 수 있다. 

### 모델의 검증 : 예측성능, 처리성능
- ``예측성능``
   - 일반적인 머신러닝 모델의 성능 지표
   - 분류냐 회귀냐의 방식에 따라서 여러가지 기준들을 확인해야 한다.
- ``처리성능``
   - 이 모델을 실제 서비스했을때 얼마나 안정적으로 빠르게 처리할 수 있느냐의 지표
   - 소프트 엔지니어들이 더 고려하는 지표
   - 실패하는 건수, 어떤 실패 유형, 성능 이슈 등
- azure : 
   - 대시보드를 통해서 서비스된 머신러닝 모델의 운영현황을 분석하고 모니터링 할 수 있다. 
   - 예측 실패요인, 퍼포먼스 현황 등의 지표들을 쉽게 확인 할 수 있다.

## ML 생애주기 모델해석 ep.4

### 모델 해석이 왜 중요한가?
- ``모델 해석 : 모델 검증 단계``
   - 모델이 복잡해질 수록 그 과정을 설명하기 어려워진다.
   - 또한 모델의 정확도만으로 모델의 성능을 평가할 수 없다. 정확도는 높은데 잘못 판단하는 예들이 발생한다.
   - 이렇게 잘못 판단한 예들을 분석하면 모델 자체가 잘못 동작하고 있다는 것을 알 수 있다. 
   - 내가 만든 모델이 잘 예측하고 있다고 착각할 수 있다.
- ``개와 늑대를 구분하는 모델``
   - 개 사진 10만장, 늑대 사진 10만장 : 학습 후 개와 늑대를 분류하는 모델을 만듦. 
   - 허스키의 경우 늑대로 잘 못 분류하는 결과 생김
   - 이러한 경우 모델 해석적 관점으로 접근할 수 있다. 모델이 왜 이렇게 분류했는가?
   - **허스키 사진에서 우리는 허스키의 얼굴을 분석할 거라고 생각하지만, 실제 모델은 얼굴이 아닌 배경만 분석함.**
   - 모델이 실제 작동하는 과정에서 어떤것에 중점을 두고 예측을하는지 아는 것이 중요함.
- ``텍스트를 분류하는 모델``
   - **학습 데이터셋에서 내가 생각한 피쳐와 모델이 중요하게 생각하는 피쳐들이 다를 수 있다.**
   - 예측을 잘못하는 상황이 생기는 것.
- ``모델 해석이 왜 중요한가?``
   - **모델 디버깅** : 왜 내 모델이 잘못 예측했나?
   - **모델 공정성 판단** : 내 모델이 알게 모르게 차별을하고 있지는 않은가?
   - **사람과 AI의 협력** : 나는 어떻게 모델의 예측을 이해하고 믿을 수 있나?
      - **성능지표들이 높다고 바로 모델을 선택하는 것이 아니라 사람이 개입해서 모델의 상황을 파악하는 과정**
   - 규제 및 컴플라이언스 : 내 모델이 법적인 요건을 만족하는가?
   - 고 리스트 영역 : 특정 산업영역에서는 모델의 성능을 설명할 수 있어야만 채택할 수 있도록 규정되어 있기도 하다.

### 모형을 해석하려는 시도
- azure 의 대시보드 설명
- x축 :
   - **local** : 내 모델이 특정 영역에서 어떻게 작동하는지 파악하는 시도, 접근방식, 알고리즘
   - **global** : 내모델이 전체적으로 어떻게 작동하는지 파악하는 시도, 접근방식, 알고리즘
- y축 :
   - **model specific**  : 특정 알고리즘, 특정 접근방식으로 만들어진 모델에 대해서 접근하는 방법
   - **model agnostic** : 제작 방식에 상관없이 범용적 모델에 대해 접근하는 방법
- 모델 해석하는 세부적인 사항들 많음

### azureml.interpret 들여다 보기
- 모델에 대해서 해석하는 기능
- 연산 결과를 볼 수 있다.
   - 모델의 특징에 따라서 분석하는 도구를 구분해 놓음
   - 설명이 쉬운 모델도 있고 어려운 모델도 있다. 좀더 설명이 쉬운 모델로 대체해서 설명해줌.

### Explainer 관련 시각화 예시
- azure 머신러닝 
   - 다수의 모델을 만들고 가장 좋은 성능을 선택
   - 이 모델을 해석하고 관련 지표들을 쉽게 보여줌
   - 모델 해석의 일반적인 범위는 global 관점인데 local 관점도 기능을 제공한다.
   - 하나의 데이터 셋을 선택한 후 어떤 피쳐가 영향을 미치는지 feature importance 값도 확인 할 수 있다.
   - 학습 데이터 전체에 대한 것이 아니라 데이터 셋 하나하나에 대한 값을 찾을 수 있다.
- 데이터 탐색
- global feature importance
- explanation
- summary importance
- feature importance 
- 추론, 예측을 할 때 필요한 데이터들 중에서 데이터들을 다른 값을 바꿨을 때 어떤 성능이 나오는지 시뮬레이션 해주는 기능
   - **perturbation 탐색** (어떤 내용인지 알아볼 것)
   - **ICE (individual conditional expectation)**
- 마이크로소프트의 Azure 서비스의 교육 사이트 : https://docs.microsoft.com/ko-kr/learn/
   - 기본적인 머신러닝 모델 개발부터 학습, 분석, 평가, 배포 등의 모든 과정에 대한 docs 있음

## MLOps 생애주기 배포/서빙 ep.5
- 배포와 서빙 단계 : 대규모 배포 준비, 배포/운영, 로깅, 모니터링

### MLOps 는 누가 하나? actor
- 모델개발 학습 : 데이터 사이언티스트
- 모델의 배포 서빙 : 데이터 엔지니어, 소프트웨어 엔지니어
- 두 그룹은 지속적인 협업이 필요함

### 일반적인 DevOps와의 비교
- DevOps : 코드, 컴파일, 테스트, 실험환경에 배포, 프로덕션 환경에 배포
- MLOps : 코드와 데이터로 모델을 개발, 프로덕션 단계에서 들어오는 데이터를 사용하여 모델 예측

### 애저머신러닝에서 패키징, 배포(서빙) 개념
- 컨테이너 기술을 이용해서 서비스를 패키징하게 된다. 
   - **모델 개발 환경을 예측 수행하는 환경에서 똑같이 재현해야 한다.**
- 모델 등록, python 패키지 정보 제공, 추론로직을 통해 예측을 수행할 것인지 지정
- **container image 생성** :  web api 방식의 서버에서 만들어진다. (docker)
   - 데이터가 실시간 혹은 batch 방식으로 들어오면 예측을 수행한다.
   - 예측으로 얻은 결과값들을 적정한 형태로 가공해서 반환해준다.
- 배포환경 지정
- 배포 / 서빙 : 패키징, 재활용 가능한 형태로 서비스 하는 단계

### 모델의 모니터링 : 데이터 트리프트
- ``모델의 성능을 모니터링하기 위한 하나의 방법``
- 모델의 예측 성능 : 실제값과 예측값 필요, 예측값은 모델의 예측결과로 얻을 수 있지만, 실제값은 현시점에 없다.
   - 결국 모델의 예측성능은 실제값과 예측값의 차이를 비교해서 얻을 수 있는데, 두 값의 시점 차이가 있어 쉽지 않다.
- ``이러한 문제를 data drift 방법을 대안으로 해결 할 수 있다.``
   - 데이터의 패턴 특성이 시간이 지나며 변하는 현상 : 학습단계의 데이터 패턴과 프로덕션 단계의 데이터 패턴이 다름
   - **학습에 활용된 당시의 데이터와 차이가 커질 수록 예측 성능이 저하될 가능성이 있다.**
   - **따라서 data drift 여부를 지속적으로 모니터링하면 모델의 재학습 시점을 판단하는데 간접적인 방법이 될 수 있다.**
      - **미리 예측모델의 성능이 떨어지는지 모니터링에 사용할 수 있다.**
- azure 머신러닝에서는 이러한 데이터 드리프트 기능을 제공함
   - 학습 데이터와 프로덕션 단계에 새로 들어오는 데이터를 적재하고 두 데이터간의 데이터 드리프트를 스케줄을 걸어 파악할 수 있다.
   - 두 데이터간의 데이터 드리프트를 분석 해준다.
   - threthold 를 지정해주면, 이 값을 넘었을 때 자동으로 경고를 주거나 다른 파이프라인을 실행할 수 있게 해준다.
   - 자동으로 재학습, 재배포 로직을 실행해서 테스트를 할 수 있다. 이 과정에서 모델의 성능을 파악, 모델 해석을 해준다. 

# 정리
- MLOps의 전반적인 과정을 통해서 머신러닝 개발과 서비스 과정에 대해서 알 수 있었다.
   - 이 과정 전체를 ML 생애주기로 생각해 볼 수 있었다.
   - 이 과정을 단순화하여 python으로 구현해 보는 것도 좋을 것 같았다.
- 특히 모델의 해석 부분에서 머신러닝 모델의 성능을 단지 정확도와 같은 지표로만 확인하는 것이 아니라, 머신러닝 모델의 작동 방식을 데이터 탐색의 수준에서 들여다 봐야 한다는 내용이 기억에 남았다.
- 또한 머신러닝 모델의 배포 단계에서 모델의 예측성능을 미리 예측하는 방법으로 데이터 드리프트라는 관점이 흥미로웠다. 
- 마이크로소프트의 Azure machine learning 서비스를 소개하는 방향으로 강의가 진행되었고, MLOps의 세밀한 단계들을 모두 관리할 수 있는 것을 보니 언젠가 서비스를 사용해 보고 싶은 생각이 들었다. 

